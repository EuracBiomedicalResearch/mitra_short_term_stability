---
title: "Differential abundance analysis, untargeted approach"
subtitle: "Positive polarity"
author: "Christa Malfertheiner"
date: "29 October 2021"
output:
  BiocStyle::html_document:
    toc: true
    number_sections: false
    toc_float: true
bibliography: references.bib
csl: biomed-central.csl
references:
- id: dummy
  title: no title
  author:
  - family: noname
    given: noname
---

```{r setup, echo = FALSE, results = "asis", warning = FALSE}
library(BiocStyle)
BiocStyle::markdown()
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
```

```{r parameters, echo = FALSE, warning = FALSE}
## Set general parameters
polarity <- "POS" # specify "POS" or "NEG"
p.cut <- 0.05     # cut-off for significance.
m.cut <- 0.7      # cut-off for log2 fold change
set.seed(123)
## Setting golden ratio to save images
phi <- (1+sqrt(5))/2
FILE_NAME <- "sts_untargeted_pos"
## Define paths:
IMAGE_PATH <- paste0("images/", FILE_NAME, "/")
if (dir.exists(IMAGE_PATH)) unlink(IMAGE_PATH, recursive = TRUE, force = TRUE)
dir.create(IMAGE_PATH, recursive = TRUE, showWarnings = FALSE)
RDATA_PATH <- paste0("data/RData/", FILE_NAME, "/")
dir.create(RDATA_PATH, recursive = TRUE, showWarnings = FALSE)
RESULT_PATH <- paste0("data/results/", FILE_NAME, "/")
dir.create(RESULT_PATH, recursive = TRUE, showWarnings = FALSE)
```

# Introduction

In this document we perform the differential abundance analysis of the features
from short term stability data collected with mitra tips, with the aim of 
finding significant storage-related features. This task is performed by 
hypothesis testing, where we try to identify which metabolites have the most
different concentrations. 
We follow an untargeted approach, the analysis comprises feature pre-filtering,
exploratory analysis and differential abundance analysis.


# Data import

First, we load the required packages and the data, after preprocessing and
normalization. The end result of these steps is a `SummarizedExperiment` that
contains aligned data, where features are grouped (after correspondence), and
that have undergone gap filling, normalization by the median, linear fitting and 
per-feature between-batch normalization to remove any unwanted variability. 
The `SummarizedExperiment` lets us store all the information regarding the 
normalization steps in the form of `assays`, which we are still able to access 
to proceed with the analysis.

```{r load-data, echo = FALSE, warning = FALSE}
library(xcms)
library(limma)
library(pheatmap)
library(writexl)
library(SummarizedExperiment)
library(RColorBrewer)
library(MsFeatures)
library(CompMetaboTools)
library(pander)
load("data/RData/sts_normalization_pos/res_pos.RData")
```

It is important now to remove the `QC` samples from the dataset, because the
analysis has to be performed only on study samples; the `QC` samples, though
are still required to evaluate the goodness of the detected features, therefore
they will be stored in a separate `SummarizedExperiment` object that can be
accessed when needed. 

We assign the colours as seen before.

```{r split-qc, echo = TRUE}
res_qc <- res_pos[, res_pos$storage == "QC"]
res_pos <- res_pos[, res_pos$storage != "QC"]

res_pos$storage <- factor(as.character(res_pos$storage))

## Define colors for the groups.
col_storage <- brewer.pal(6, name = "Set1")
names(col_storage) <- c("4C_BAG",   # red
                        "RT",        # blue
                        "BAG",       # green
                        "VACUUM",    # purple
                        "4C_VACUUM", # orange
                        "QC")        # yellow
col_time <- brewer.pal(8, name = "Set3")
names(col_time) <- c("1d",
                     "1w",
                     "2d",
                     "2h",
                     "3d",
                     "6h",
                     "2w",
                     "QC")

## Setting golden ratio to save images
phi <- (1+sqrt(5))/2
```

The samples used in this analysis are listed below.

```{r, echo = FALSE, results = "asis"}
tab <- colData(res_pos)[, c("storage", "time")]
pandoc.table(as.data.frame(tab), style = "rmarkdown",
             caption = "Samples used in this analysis")
```

# Feature pre-filtering

Feature pre-filtering is an important step of data analysis that aims to
reduce as much as possible the random error that occurs during the measurement
of an analyte: during this process, features with high noise and features that
were detected in a low number of samples are removed. As a side effect, by
reducing the number of features that are being tested later, the pre-filtering
reduces also the loss of power by the subsequent adjustment for multiple
hypothesis testing.

This step is fundamental, though one must be careful not to pre-filter for a
characteristic that will be tested later: the pre-filtering must be, as a matter
of fact, independent from later analyses.

The first step of pre-filtering consists of removing features with high
technical variance; several methods have been developed to determine which
signals must be removed, the most common of which relies on the **relative
standard deviation (RSD)**, which is defined as the ratio between the standard
deviation and the mean:

$RSD = \dfrac{s_{i,qc}}{\bar{m}_{i,qc}}$

This value is calculated for each feature found in the pooled QC samples: when
this is higher than 30%, the feature is removed from the dataset
[@broadhurstGuidelinesConsiderationsUse2018].

Another common approach is based on the **dispersion ratio (D-ratio)**: this is
defined as the ratio between the sample standard deviation for the pooled QC
samples and the sample standard deviation for the study samples (the former
expected to represent technical variance, the latter a combination of technical
and biological variance):

$D-ratio = \dfrac{s_{i,qc}}{s_{i,sample}}$

The interpretation of this value goes as follows: when the D-ratio is 0%, there
is no technical variance in the observed measurements, whereas a D-ratio of
100% represents only noise and no biological variance detected. A common cut-off
for the D-ratio is 0.5, aiming at keeping features whose variation in study
samples is twice as large as the one in QC samples
[@broadhurstGuidelinesConsiderationsUse2018]. 

For the present dataset calculate the D-ratio **separately** for each source
(sample matrix) because the variance between sample matrices is expected to be
very large. For each feature the mean D-ratio across the 3 sample matrices is
used for the filtering.

```{r filter-rsd}
rsds <- rowRsd(assay(res_qc, "normalized_filled"))
dratios <- apply(
    log2(assay(res_qc, "normalized_filled")), 1, sd, na.rm = TRUE) /
    apply(log2(assay(res_pos, "normalized_filled")), 1, sd, na.rm = TRUE)
```

The distribution of RSD values and D-ratio is shown in the plot below:

```{r filter-rsd-plot, fig.cap = "Distribution of RSD values and D-ratios in the data set. The dashed vertical red line represents the cut-off value for the RSD and D-ratio, respectively.", echo = FALSE}
par(mfrow = c(1, 2), mar = c(5, 5, 5, 1))
plot(density(rsds, na.rm = TRUE), xlab = "RSD",
     main = "Distribution of RSD values",
     cex.main = 1.5, cex.lab = 1.5, cex.axis = 1.3)
abline(v = 0.3, col = "red", lty = 2)
plot(density(dratios, na.rm = TRUE), xlab = "D-ratio",
     main = "Distribution of D-ratios",
     cex.main = 1.5, cex.lab = 1.5, cex.axis = 1.3)
abline(v = 0.5, col = "red", lty = 2)
```

The plot below directly compares the RSD and D-ratio for each feature.

```{r filter-rsd-vs-dratio-plot, fig.path = IMAGE_PATH, fig.width = 5, fig.height = 5, fig.cap = "Direct comparison of RSD and D-ratios.", echo = FALSE}
plot(log2(rsds), log2(dratios), xlab = expression(log[2]~RSD),
     ylab = expression(log[2]~D-ratio), pch = 16, col = "#00000040")
abline(v = log2(0.3), col = "red", lty = 2)
abline(h = log2(0.5), col = "red", lty = 2)
```

The plot shows a correlation between RSD and D-ratios, though the two methods
are not interchangeable. Below we pre-filter the data using the D-ratio.

```{r do-filter}
res <- res_pos[which(dratios < 0.5), ]
```

This reduced the data set from `r length(dratios)` to `r nrow(res)` features.
Next, we discard the features that have not been identified in at least one
third of the samples in any of the sample groups.

```{r filter-proportion}
keep <- moreAreValidThan(assay(res, "raw"), f = res$storage, prop = 1/3)
res <- res[keep, ]
```

The dataset has been reduced from `r length(rsds)` to `r nrow(res)` features:
this result shows that most features have been retained even after
pre-filtering, thus ensuring a dataset where features have a D-ratio lower than
0.5 and have less than 70% missing values.


# Exploratory analysis: PCA

Next, we perform a PCA analysis: this allows us to gather information about any
possible similarities among the samples, based on the measured metabolite
intensities.

```{r pca-all}
pc <- prcomp(t(log2(assay(res, "normalized_filled_imputed"))),
             center = TRUE, scale. = FALSE)
```

```{r pca-plot, fig.path = IMAGE_PATH, fig.cap = "PCA of the samples based on feature intensities.", fig.width = 7 * phi, fig.height = 7, echo = FALSE}
par(mfrow = c(1, 2))
plot_pca(pc, col = paste0(col_storage[as.character(res$storage)], 90),
         pc_x = 1, pc_y = 2)
plot_pca(pc, col = paste0(col_storage[as.character(res$storage)], 90),
         pc_x = 3, pc_y = 4)
legend("topleft", col_storage, legend = names(col_storage),
       title = "phenotype", pch = 16, ncol = 2)
```

Add description

# Differential abundance analysis

In this section, we perform a differential abundance analysis to identify
features that have significantly different abundances between male and female 
samples. The analysis is based on feature-wise multiple linear regression:
the aim of such analysis is to find the relationship between the independent
variables (age and sex) and the response variable (signal intensity).
In short, multiple linear regression is a form of linear regression that is used
when there are two or more predictors.
Multiple linear regression is preferred over separate simple linear regression
in order to avoid wrong predictions: this could happen because the input
variables may be correlated, which could lead to unsatisfactory results. The
formula for multiple regression model is:

$Y = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} \ldots, + \beta_px_{ip} + \epsilon$

where: \
- $Y =$ predicted value (dependent variable) \
- $\beta_0 =$ y intercept, constant term
- $\beta_1, \beta_2, \ldots, \beta_p =$ regression coefficients \
- $x_i =$ independent variables \
- $\epsilon =$ residuals.

The `limma` package contains the `lmFit` function, which calculates the linear
model that best describes the data. The results are stored in a `MArrayLM`
(Microarray Linear Model Fit) object.

This model, though, is not enough to accept whether or not there is actually a
relationship among the response and the independent variables, therefore we must
perform a hypothesis test: we define the *null hypothesis* as there not being
any differences in the abundances of metabolites between the different storage 
methods. The *alternative hypothesis* is therefore defined when there are 
differences in the intensities of the signals coming from the metabolites in 
the different experimental groups.
To accept or reject the alternative hypothesis, it is necessary to calculate the
p-value: the function that allows us to add the results to the `MArrayLM` object
created before is `eBayes`. The `eBayes` function computes several statistics,
including the moderated t-test, which is defined as follows:

$\dfrac{d}{s + s_0}$

where \
- $d =$ difference in two group means ($m_1 - m_2$) \
- $s =$ pooled standard deviation \
- $s_0 =$ small constant (it depends on the variance within the group).

The constant is added to the denominator in order to avoid a division by an
extremely low number, which would of course increase the result of the statistic
falsely inducing us into rejecting the null hypothesis, thus considering the
difference as significant, when it is not. When performing multiple hypothesis
testing, though, there is a high chance of rejecting the null hypothesis when it
is true (type I error), thus a method to control the False Discovery Rate is
required: in this case we opted for Benjamini-Hochberg correction. In
conclusion, the alternative hypothesis is rejected when the adjusted p-value is
smaller than the confidence threshold that was set at the beginning of this
document to `r p.cut`. This means we accept `r p.cut * 100`% false positives
among the features called *significant*.

```{r analysis}
## Factor sample source, sex and age
storage <- factor(res$storage)
time <- factor(res$time)
storetime <- factor(res$storage_time)

## Fit the data to the desired design
dsgn <- model.matrix(~ 0 + storetime)
fit <- lmFit(log2(assay(res, "normalized_filled_imputed")), design = dsgn)

## Fit the actual contrasts of interest
contr_mat <- makeContrasts(
  RT2hvsRT6h = storetimeRT_2h - storetimeRT_6h,
  RT2hvsRT1d = storetimeRT_2h - storetimeRT_1d,
  RT2hvsRT2d = storetimeRT_2h - storetimeRT_2d,
  RT2hvsRT3d = storetimeRT_2h - storetimeRT_3d,
  RT2hvsRT1w = storetimeRT_2h - storetimeRT_1w,
  RT2hvsRT2w = storetimeRT_2h - storetimeRT_2w,
  RT2hvsBAG6h = storetimeRT_2h - storetimeBAG_6h,
  RT2hvsBAG1d = storetimeRT_2h - storetimeBAG_1d,
  RT2hvsBAG2d = storetimeRT_2h - storetimeBAG_2d,
  RT2hvsBAG3d = storetimeRT_2h - storetimeBAG_3d,
  RT2hvsBAG1w = storetimeRT_2h - storetimeBAG_1w,
  RT2hvsBAG2w = storetimeRT_2h - storetimeBAG_2w,
  RT2hvsVACUUM6h = storetimeRT_2h - storetimeVACUUM_6h,
  RT2hvsVACUUM1d = storetimeRT_2h - storetimeVACUUM_1d,
  RT2hvsVACUUM2d = storetimeRT_2h - storetimeVACUUM_2d,
  RT2hvsVACUUM3d = storetimeRT_2h - storetimeVACUUM_3d,
  RT2hvsVACUUM1w = storetimeRT_2h - storetimeVACUUM_1w,
  RT2hvsVACUUM2w = storetimeRT_2h - storetimeVACUUM_2w,
  RT2hvs4CBAG6h = storetimeRT_2h - storetime4C_BAG_6h,
  RT2hvs4CBAG1d = storetimeRT_2h - storetime4C_BAG_1d,
  RT2hvs4CBAG2d = storetimeRT_2h - storetime4C_BAG_2d,
  RT2hvs4CBAG3d = storetimeRT_2h - storetime4C_BAG_3d,
  RT2hvs4CBAG1w = storetimeRT_2h - storetime4C_BAG_1w,
  RT2hvs4CBAG2w = storetimeRT_2h - storetime4C_BAG_2w,
  RT2hvs4CVACUUM6h = storetimeRT_2h - storetime4C_VACUUM_6h,
  RT2hvs4CVACUUM1d = storetimeRT_2h - storetime4C_VACUUM_1d,
  RT2hvs4CVACUUM2d = storetimeRT_2h - storetime4C_VACUUM_2d,
  RT2hvs4CVACUUM3d = storetimeRT_2h - storetime4C_VACUUM_3d,
  RT2hvs4CVACUUM1w = storetimeRT_2h - storetime4C_VACUUM_1w,
  RT2hvs4CVACUUM2w = storetimeRT_2h - storetime4C_VACUUM_2w,
  levels = dsgn)
fit <- contrasts.fit(fit, contrasts = contr_mat)
fit <- eBayes(fit)
adjp <- apply(fit$p.value, 2, p.adjust, method = "BH")
tmp <- data.frame(
    coef = fit$coefficient,
    pvalue = fit$p.value,
    adjp = adjp,
    significant = adjp < p.cut & abs(fit$coefficient) > m.cut
)
tmp$avg.RT2h <- rowMeans(log2(assay(res, "normalized_filled_imputed")
                              [, res$storage_time == "RT_2h"]))
tmp$avg.RT6h <- rowMeans(log2(assay(res, "normalized_filled_imputed")
                              [, res$storage_time == "RT_6h"]))
tmp$avg.RT1d <- rowMeans(log2(assay(res, "normalized_filled_imputed")
                              [, res$storage_time == "RT_1d"]))
tmp$avg.RT2d <- rowMeans(log2(assay(res, "normalized_filled_imputed")
                              [, res$storage_time == "RT_2d"]))
tmp$avg.RT3d <- rowMeans(log2(assay(res, "normalized_filled_imputed")
                              [, res$storage_time == "RT_3d"]))
tmp$avg.RT1w <- rowMeans(log2(assay(res, "normalized_filled_imputed")
                              [, res$storage_time == "RT_1w"]))
tmp$avg.RT2w <- rowMeans(log2(assay(res, "normalized_filled_imputed")
                              [, res$storage_time == "RT_2w"]))
tmp$avg.BAG6h <- rowMeans(log2(assay(res, "normalized_filled_imputed")
                              [, res$storage_time == "BAG_6h"]))
tmp$avg.BAG1d <- rowMeans(log2(assay(res, "normalized_filled_imputed")
                              [, res$storage_time == "BAG_1d"]))
tmp$avg.BAG2d <- rowMeans(log2(assay(res, "normalized_filled_imputed")
                              [, res$storage_time == "BAG_2d"]))
tmp$avg.BAG3d <- rowMeans(log2(assay(res, "normalized_filled_imputed")
                              [, res$storage_time == "BAG_3d"]))
tmp$avg.BAG1w <- rowMeans(log2(assay(res, "normalized_filled_imputed")
                              [, res$storage_time == "BAG_1w"]))
tmp$avg.BAG2w <- rowMeans(log2(assay(res, "normalized_filled_imputed")
                              [, res$storage_time == "BAG_2w"]))
tmp$avg.VACUUM6h <- rowMeans(log2(assay(res, "normalized_filled_imputed")
                              [, res$storage_time == "VACUUM_6h"]))
tmp$avg.VACUUM1d <- rowMeans(log2(assay(res, "normalized_filled_imputed")
                              [, res$storage_time == "VACUUM_1d"]))
tmp$avg.VACUUM2d <- rowMeans(log2(assay(res, "normalized_filled_imputed")
                              [, res$storage_time == "VACUUM_2d"]))
tmp$avg.VACUUM3d <- rowMeans(log2(assay(res, "normalized_filled_imputed")
                              [, res$storage_time == "VACUUM_3d"]))
tmp$avg.VACUUM1w <- rowMeans(log2(assay(res, "normalized_filled_imputed")
                              [, res$storage_time == "VACUUM_1w"]))
tmp$avg.VACUUM2w <- rowMeans(log2(assay(res, "normalized_filled_imputed")
                              [, res$storage_time == "VACUUM_2w"]))
tmp$avg.4CBAG6h <- rowMeans(log2(assay(res, "normalized_filled_imputed")
                              [, res$storage_time == "4C_BAG_6h"]))
tmp$avg.4CBAG1d <- rowMeans(log2(assay(res, "normalized_filled_imputed")
                              [, res$storage_time == "4C_BAG_1d"]))
tmp$avg.4CBAG2d <- rowMeans(log2(assay(res, "normalized_filled_imputed")
                              [, res$storage_time == "4C_BAG_2d"]))
tmp$avg.4CBAG3d <- rowMeans(log2(assay(res, "normalized_filled_imputed")
                              [, res$storage_time == "4C_BAG_3d"]))
tmp$avg.4CBAG1w <- rowMeans(log2(assay(res, "normalized_filled_imputed")
                              [, res$storage_time == "4C_BAG_1w"]))
tmp$avg.4CBAG2w <- rowMeans(log2(assay(res, "normalized_filled_imputed")
                              [, res$storage_time == "4C_BAG_2w"]))
tmp$avg.4CVACUUM6h <- rowMeans(log2(assay(res, "normalized_filled_imputed")
                              [, res$storage_time == "4C_VACUUM_6h"]))
tmp$avg.4CVACUUM1d <- rowMeans(log2(assay(res, "normalized_filled_imputed")
                              [, res$storage_time == "4C_VACUUM_1d"]))
tmp$avg.4CVACUUM2d <- rowMeans(log2(assay(res, "normalized_filled_imputed")
                              [, res$storage_time == "4C_VACUUM_2d"]))
tmp$avg.4CVACUUM3d <- rowMeans(log2(assay(res, "normalized_filled_imputed")
                              [, res$storage_time == "4C_VACUUM_3d"]))
tmp$avg.4CVACUUM1w <- rowMeans(log2(assay(res, "normalized_filled_imputed")
                              [, res$storage_time == "4C_VACUUM_1w"]))
tmp$avg.4CVACUUM2w <- rowMeans(log2(assay(res, "normalized_filled_imputed")
                              [, res$storage_time == "4C_VACUUM_2w"]))
rowData(res) <- cbind(rowData(res), tmp)
```

We plot then the distribution of p-values, both raw and adjusted for all 
samples stored at room temperature:

```{r standards-p-value-histogram, echo = FALSE, fig.path = IMAGE_PATH, fig.width = 7 * phi, fig.cap = "Distribution of raw (left) and adjusted p-values (right) for all 6 comparisons."}
par(mfrow = c(3, 4))
hist(rowData(res)$pvalue.RT2hvsRT6h, breaks = 64, xlab = "p value",
     main = "RT for 2h vs RT for 6h")
hist(rowData(res)$adjp.RT2hvsRT6h, breaks = 64,
     xlab = expression(p[BH]~value),
     main = "RT for 2h vs RT for 6h")
hist(rowData(res)$pvalue.RT2hvsRT1d, breaks = 64, xlab = "p value",
     main = "RT for 2h vs RT for 1d")
hist(rowData(res)$adjp.RT2hvsRT1d, breaks = 64,
     xlab = expression(p[BH]~value),
     main = "RT for 2h vs RT for 1d")
hist(rowData(res)$pvalue.RT2hvsRT2d, breaks = 64, xlab = "p value",
     main = "RT for 2h vs RT for 2d")
hist(rowData(res)$adjp.RT2hvsRT2d, breaks = 64,
     xlab = expression(p[BH]~value),
     main = "RT for 2h vs RT for 2d")
hist(rowData(res)$pvalue.RT2hvsRT3d, breaks = 64, xlab = "p value",
     main = "RT for 2h vs RT for 3d")
hist(rowData(res)$adjp.RT2hvsRT3d, breaks = 64,
     xlab = expression(p[BH]~value),
     main = "RT for 2h vs RT for 3d")
hist(rowData(res)$pvalue.RT2hvsRT1w, breaks = 64, xlab = "p value",
     main = "RT for 2h vs RT for 1w")
hist(rowData(res)$adjp.RT2hvsRT1w, breaks = 64,
     xlab = expression(p[BH]~value),
     main = "RT for 2h vs RT for 1w")
hist(rowData(res)$pvalue.RT2hvsRT2w, breaks = 64, xlab = "p value",
     main = "RT for 2h vs RT for 2w")
hist(rowData(res)$adjp.RT2hvsRT2w, breaks = 64,
     xlab = expression(p[BH]~value),
     main = "RT for 2h vs RT for 2w")

```

Then, we plot distribution of the raw and adjusted p-values for the comparison
of samples stored in plastic bags and the sample that dried for 2h at room
temperature:

```{r standards-p-value-histogram, echo = FALSE, fig.path = IMAGE_PATH, fig.width = 7 * phi, fig.cap = "Distribution of raw (left) and adjusted p-values (right) for all 6 comparisons."}
par(mfrow = c(3, 4))
hist(rowData(res)$pvalue.RT2hvsBAG6h, breaks = 64, xlab = "p value",
     main = "RT for 2h vs BAG for 6h")
hist(rowData(res)$adjp.RT2hvsBAG6h, breaks = 64,
     xlab = expression(p[BH]~value),
     main = "RT for 2h vs BAG for 6h")
hist(rowData(res)$pvalue.RT2hvsBAG1d, breaks = 64, xlab = "p value",
     main = "RT for 2h vs BAG for 1d")
hist(rowData(res)$adjp.RT2hvsBAG1d, breaks = 64,
     xlab = expression(p[BH]~value),
     main = "RT for 2h vs BAG for 1d")
hist(rowData(res)$pvalue.RT2hvsBAG2d, breaks = 64, xlab = "p value",
     main = "RT for 2h vs BAG for 2d")
hist(rowData(res)$adjp.RT2hvsBAG2d, breaks = 64,
     xlab = expression(p[BH]~value),
     main = "RT for 2h vs BAG for 2d")
hist(rowData(res)$pvalue.RT2hvsBAG3d, breaks = 64, xlab = "p value",
     main = "RT for 2h vs BAG for 3d")
hist(rowData(res)$adjp.RT2hvsBAG3d, breaks = 64,
     xlab = expression(p[BH]~value),
     main = "RT for 2h vs BAG for 3d")
hist(rowData(res)$pvalue.RT2hvsBAG1w, breaks = 64, xlab = "p value",
     main = "RT for 2h vs BAG for 1w")
hist(rowData(res)$adjp.RT2hvsBAG1w, breaks = 64,
     xlab = expression(p[BH]~value),
     main = "RT for 2h vs BAG for 1w")
hist(rowData(res)$pvalue.RT2hvsBAG2w, breaks = 64, xlab = "p value",
     main = "RT for 2h vs BAG for 2w")
hist(rowData(res)$adjp.RT2hvsBAG2w, breaks = 64,
     xlab = expression(p[BH]~value),
     main = "RT for 2h vs BAG for 2w")

```

Next, we plot distribution of the raw and adjusted p-values for the comparison
of samples stored in vacuum bags and the sample that dried for 2h at room
temperature:

```{r standards-p-value-histogram, echo = FALSE, fig.path = IMAGE_PATH, fig.width = 7 * phi, fig.cap = "Distribution of raw (left) and adjusted p-values (right) for all 6 comparisons."}
par(mfrow = c(3, 4))
hist(rowData(res)$pvalue.RT2hvsVACUUM6h, breaks = 64, xlab = "p value",
     main = "RT for 2h vs VAC for 6h")
hist(rowData(res)$adjp.RT2hvsVACUUM6h, breaks = 64,
     xlab = expression(p[BH]~value),
     main = "RT for 2h vs VAC for 6h")
hist(rowData(res)$pvalue.RT2hvsVACUUM1d, breaks = 64, xlab = "p value",
     main = "RT for 2h vs VAC for 1d")
hist(rowData(res)$adjp.RT2hvsVACUUM1d, breaks = 64,
     xlab = expression(p[BH]~value),
     main = "RT for 2h vs VAC for 1d")
hist(rowData(res)$pvalue.RT2hvsVACUUM2d, breaks = 64, xlab = "p value",
     main = "RT for 2h vs VAC for 2d")
hist(rowData(res)$adjp.RT2hvsVACUUM2d, breaks = 64,
     xlab = expression(p[BH]~value),
     main = "RT for 2h vs VAC for 2d")
hist(rowData(res)$pvalue.RT2hvsVACUUM3d, breaks = 64, xlab = "p value",
     main = "RT for 2h vs VAC for 3d")
hist(rowData(res)$adjp.RT2hvsVACUUM3d, breaks = 64,
     xlab = expression(p[BH]~value),
     main = "RT for 2h vs VAC for 3d")
hist(rowData(res)$pvalue.RT2hvsVACUUM1w, breaks = 64, xlab = "p value",
     main = "RT for 2h vs VAC for 1w")
hist(rowData(res)$adjp.RT2hvsVACUUM1w, breaks = 64,
     xlab = expression(p[BH]~value),
     main = "RT for 2h vs VAC for 1w")
hist(rowData(res)$pvalue.RT2hvsVACUUM2w, breaks = 64, xlab = "p value",
     main = "RT for 2h vs VAC for 2w")
hist(rowData(res)$adjp.RT2hvsVACUUM2w, breaks = 64,
     xlab = expression(p[BH]~value),
     main = "RT for 2h vs VAC for 2w")

```

Then, we plot distribution of the raw and adjusted p-values for the comparison
of samples stored in plastic bags at 4°C and the sample that dried for 2h at
room temperature:

```{r standards-p-value-histogram, echo = FALSE, fig.path = IMAGE_PATH, fig.width = 7 * phi, fig.cap = "Distribution of raw (left) and adjusted p-values (right) for all 6 comparisons."}
par(mfrow = c(3, 4))
hist(rowData(res)$pvalue.RT2hvs4CBAG6h, breaks = 64, xlab = "p value",
     main = "RT for 2h vs 4C BAG for 6h")
hist(rowData(res)$adjp.RT2hvs4CBAG6h, breaks = 64,
     xlab = expression(p[BH]~value),
     main = "RT for 2h vs 4C BAG for 6h")
hist(rowData(res)$pvalue.RT2hvs4CBAG1d, breaks = 64, xlab = "p value",
     main = "RT for 2h vs 4C BAG for 1d")
hist(rowData(res)$adjp.RT2hvs4CBAG1d, breaks = 64,
     xlab = expression(p[BH]~value),
     main = "RT for 2h vs 4C BAG for 1d")
hist(rowData(res)$pvalue.RT2hvs4CBAG2d, breaks = 64, xlab = "p value",
     main = "RT for 2h vs 4C BAG for 2d")
hist(rowData(res)$adjp.RT2hvs4CBAG2d, breaks = 64,
     xlab = expression(p[BH]~value),
     main = "RT for 2h vs 4C BAG for 2d")
hist(rowData(res)$pvalue.RT2hvs4CBAG3d, breaks = 64, xlab = "p value",
     main = "RT for 2h vs 4C BAG for 3d")
hist(rowData(res)$adjp.RT2hvs4CBAG3d, breaks = 64,
     xlab = expression(p[BH]~value),
     main = "RT for 2h vs 4C BAG for 3d")
hist(rowData(res)$pvalue.RT2hvs4CBAG1w, breaks = 64, xlab = "p value",
     main = "RT for 2h vs 4C BAG for 1w")
hist(rowData(res)$adjp.RT2hvs4CBAG1w, breaks = 64,
     xlab = expression(p[BH]~value),
     main = "RT for 2h vs 4C BAG for 1w")
hist(rowData(res)$pvalue.RT2hvs4CBAG2w, breaks = 64, xlab = "p value",
     main = "RT for 2h vs 4C BAG for 2w")
hist(rowData(res)$adjp.RT2hvs4CBAG2w, breaks = 64,
     xlab = expression(p[BH]~value),
     main = "RT for 2h vs 4C BAG for 2w")

```

Lastly, we plot distribution of the raw and adjusted p-values for the comparison
of samples stored in vacuum bags at 4°c and the sample that dried for 2h at room
temperature:

```{r standards-p-value-histogram, echo = FALSE, fig.path = IMAGE_PATH, fig.width = 10 * phi, fig.cap = "Distribution of raw (left) and adjusted p-values (right) for all 6 comparisons."}
par(mfrow = c(3, 4))
hist(rowData(res)$pvalue.RT2hvs4CVACUUM6h, breaks = 64, xlab = "p value",
     main = "RT for 2h vs 4C VAC for 6h")
hist(rowData(res)$adjp.RT2hvs4CVACUUM6h, breaks = 64,
     xlab = expression(p[BH]~value),
     main = "RT for 2h vs 4C VAC for 6h")
hist(rowData(res)$pvalue.RT2hvs4CVACUUM1d, breaks = 64, xlab = "p value",
     main = "RT for 2h vs 4C VAC for 1d")
hist(rowData(res)$adjp.RT2hvs4CVACUUM1d, breaks = 64,
     xlab = expression(p[BH]~value),
     main = "RT for 2h vs 4C VAC for 1d")
hist(rowData(res)$pvalue.RT2hvs4CVACUUM2d, breaks = 64, xlab = "p value",
     main = "RT for 2h vs 4C VAC for 2d")
hist(rowData(res)$adjp.RT2hvs4CVACUUM2d, breaks = 64,
     xlab = expression(p[BH]~value),
     main = "RT for 2h vs 4C VAC for 2d")
hist(rowData(res)$pvalue.RT2hvs4CVACUUM3d, breaks = 64, xlab = "p value",
     main = "RT for 2h vs 4C VAC for 3d")
hist(rowData(res)$adjp.RT2hvs4CVACUUM3d, breaks = 64,
     xlab = expression(p[BH]~value),
     main = "RT for 2h vs 4C VAC for 3d")
hist(rowData(res)$pvalue.RT2hvs4CVACUUM1w, breaks = 64, xlab = "p value",
     main = "RT for 2h vs 4C VAC for 1w")
hist(rowData(res)$adjp.RT2hvs4CVACUUM1w, breaks = 64,
     xlab = expression(p[BH]~value),
     main = "RT for 2h vs 4C VAC for 1w")
hist(rowData(res)$pvalue.RT2hvs4CVACUUM2w, breaks = 64, xlab = "p value",
     main = "RT for 2h vs 4C VAC for 2w")
hist(rowData(res)$adjp.RT2hvs4CVACUUM2w, breaks = 64,
     xlab = expression(p[BH]~value),
     main = "RT for 2h vs 4C VAC for 2w")

```

Thus, most of the metabolites were identified to have significantly different
concentrations between the comparison. A table with the number of significant
metabolites is shown below.

```{r table-sig, echo = FALSE, results = "asis"}
tab <- colSums(as.matrix(rowData(res)[, grep("significant", 
                                             colnames(rowData(res)))]))
pandoc.table(tab, style = "rmarkdown",
             caption = paste0("Number of significant features of the in",
                              " total", nrow(res), "analyzed features."))
```

The number of significant features is then shown in a barplot:

```{r sig-features-barplot, echo = FALSE, fig.path = IMAGE_PATH, fig.width = 10 * phi, fig.cap = "Amount of significant features per storage condition over time"}

pd <- read.table("data/sts_untar_sig.txt", sep = "\t", as.is = TRUE, header = TRUE)
library("ggplot2")
ggplot(pd, aes(fill=storage, y=significant, x=time)) + 
    geom_bar(position="dodge", stat="identity") +
  scale_fill_brewer(palette="Set1")
```
Samples, that were stored at room temperature, show the smallest changes in
the features compared to the samples, that were froze directly after drying.
The samples that were stored in plastic bags at 4°C show the highest number
of significant features compared to samples frozen after drying after one day,
whereas after two weeks the samples stored in plastic bags at room temperature
show th highest peak. Interestingly, after one week the amount of significant
features was lower in all conditions compared to the amount after 3 days.

# Session information

```{r}
sessionInfo()
```
